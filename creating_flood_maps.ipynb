{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f64a3fea",
   "metadata": {},
   "source": [
    "## In this notebook, we create flood maps which will serve as target for training a flood mapping simulator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e4754a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Eric\\climax\\floods_diffusion\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import natsort\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "import time\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.mask import mask\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.features import rasterize\n",
    "import os\n",
    "from shapely.geometry import Polygon, Point\n",
    "from shapely.ops import unary_union\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from dask import delayed, compute\n",
    "from tqdm import tqdm\n",
    "from natsort import natsorted\n",
    "import gc\n",
    "from PIL import Image, ImageDraw, ImageFont # Import specific modules from PIL\n",
    "import requests\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import geojson\n",
    "from scipy.ndimage import binary_dilation\n",
    "import pandas as pd\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b21d44d",
   "metadata": {},
   "source": [
    "### Define a bunch of helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7be7483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- Helper function to get tile name from lat/lon\n",
    "def tile_name_from_latlon(lat, lon):\n",
    "\n",
    "    \"\"\"\n",
    "    Example usage:\n",
    "    Nairobi coordinates: 0.0236° S, 37.9062° E\n",
    "    print(f\"Nairobi tile: {tile_name_from_latlon(lat = 1.29, lon = 36.82)}\")\n",
    "    \"\"\"\n",
    "\n",
    "    # Data is in 3 x 3 degree tiles so\n",
    "     \n",
    "    lat_tile = (lat // 3) * 3\n",
    "    lon_tile = (lon // 3) * 3\n",
    "    lat_tile, lon_tile = int(lat_tile), int(lon_tile)\n",
    "\n",
    "    lat_prefix = 'N' if lat_tile >= 0 else 'S'\n",
    "    lon_prefix = 'E' if lon_tile >= 0 else 'W' \n",
    "\n",
    "    return f\"{lat_prefix}{abs(lat_tile):02d}{lon_prefix}{abs(lon_tile):03d}\"\n",
    "\n",
    "\n",
    "#--------------------------- Function to download tiles from Hugging Face\n",
    "def download_tile(tile,\n",
    "                  repo_id = \"ai-for-good-lab/ai4g-flood-dataset\",\n",
    "                  download_240m_buffer_tif = True,\n",
    "                  download_80m_buffer_tif = True,\n",
    "                  download_post_processing_parquet = True,\n",
    "                  download_recurrence_80m_buffer_tif = True,\n",
    "                  local_dir = None,\n",
    "                  overwrite = True\n",
    "                  ):\n",
    "    \n",
    "    print(f\"\\nDownloading data for tile: {tile}\")\n",
    "    \n",
    "    if local_dir is None:\n",
    "        local_dir = Path(f\"flood_data\")\n",
    "\n",
    "    if not local_dir.exists():\n",
    "        local_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    root_dir = tile[:3]\n",
    "    files_to_download = []\n",
    "    if download_240m_buffer_tif:\n",
    "        files_to_download.append(f\"{root_dir}/{tile}/{tile}-240m-buffer.tif\")\n",
    "\n",
    "    if download_80m_buffer_tif:\n",
    "        files_to_download.append(f\"{root_dir}/{tile}/{tile}-80m-buffer.tif\")\n",
    "\n",
    "    if download_post_processing_parquet:\n",
    "        files_to_download.append(f\"{root_dir}/{tile}/{tile}-post-processing.parquet\")\n",
    "\n",
    "    if download_recurrence_80m_buffer_tif:\n",
    "        files_to_download.append(f\"{root_dir}/{tile}/{tile}-recurrence-80m-buffer.tif\")\n",
    "\n",
    "    for file_path in files_to_download:\n",
    "        local_file_path = local_dir / Path(file_path).name\n",
    "        if not local_file_path.exists() or overwrite:\n",
    "            print(f\"Downloading {file_path} to {local_file_path}\")\n",
    "            hf_hub_download(repo_id = repo_id,\n",
    "                            filename = file_path,\n",
    "                            local_dir = str(local_dir),\n",
    "                            repo_type = \"dataset\",\n",
    "            )\n",
    "        else:\n",
    "            print(f\"File {local_file_path} already exists. Skipping download.\")\n",
    "\n",
    "\n",
    "    return\n",
    "\n",
    "# ----------------- Helper function to produce normalized rgb values\n",
    "def rgb(r, g, b):\n",
    "    return (r / 255, g / 255, b / 255)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------- Helper function to get administrative boundary from GeoBoundaries\n",
    "def get_admin_boundary(iso = 'KEN', adm = 'ADM0', save_path = None):\n",
    "    \n",
    "    # Query the GeoBoundaries API for the specified boundary\n",
    "    url = f\"https://www.geoboundaries.org/api/current/gbOpen/{iso}/{adm}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Error fetching data from GeoBoundaries API: {response.status_code}\")\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    boundary_url = data['gjDownloadURL']\n",
    "\n",
    "    # Download the GeoJSON boundary file\n",
    "    geoboundary = gpd.read_file(boundary_url)\n",
    "\n",
    "    if save_path is not None:\n",
    "        save_path = Path(save_path)/f\"{iso}_{adm}_boundary.geojson\"\n",
    "        geoboundary.to_file(save_path, driver='GeoJSON')\n",
    "        print(f\"Saved boundary to {save_path}\")\n",
    "\n",
    "    return geoboundary\n",
    "\n",
    "# --------------------- Function to create thumbnail images from tif files\n",
    "# cmap = ListedColormap(['lightgrey', 'orange', 'blue']) # 0, 1, 2\n",
    "# Color maps inspired by Amit's work: https://arxiv.org/pdf/2411.01411\n",
    "cmap = ListedColormap(\n",
    "    [\n",
    "        rgb(241, 241, 241), # 0 - light grey - no data\n",
    "        rgb(208, 207, 212), # 1 - Exclusion mask\n",
    "        rgb(0, 0, 255)    # 2 - Flooded - blue\n",
    "\n",
    "    ]\n",
    ")\n",
    "norm = BoundaryNorm([-0.5,0.5,1.5,2.5], cmap.N)\n",
    "def make_thumbnail(\n",
    "    tif_paths,\n",
    "    idx,\n",
    "    png_dir,\n",
    "    max_size = 1024,\n",
    "    geoboundary_gdf = None,\n",
    "    boundary_color = [0, 0, 0], # Black boundary\n",
    "    boundary_width = 1,\n",
    "    cmap = cmap,\n",
    "    norm = norm,\n",
    "\n",
    "    title = None,\n",
    "    title_color = 'black',\n",
    "    title_font_size = 24,\n",
    "    font_path='arial.ttf', # Path to a .ttf font file\n",
    "\n",
    "    update_bar = None,\n",
    "):\n",
    "    tif_path = tif_paths[idx]\n",
    "    print(f\"\\nProcessing {tif_path}...\")\n",
    "\n",
    "    # Read and downsample\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        raster_crs = src.crs\n",
    "        h, w = src.height, src.width\n",
    "        scale = max(h, w) / max_size\n",
    "\n",
    "        out_h = max(1, int(h / scale))\n",
    "        out_w = max(1, int(w / scale))\n",
    "\n",
    "        arr = src.read(\n",
    "            1,\n",
    "            out_shape = (out_h, out_w),\n",
    "            resampling = Resampling.nearest\n",
    "        )\n",
    "        new_transform = src.transform * src.transform.scale(\n",
    "            (src.width / out_w),\n",
    "            (src.height / out_h)\n",
    "        )\n",
    "\n",
    "    print(f\"Downsampling by scale factor: {scale:.2f}\")\n",
    "    print(f\"Original resolution in km: {src.transform[0]*111.32:.3f}, New resolution in km: {new_transform[0]*111.32:.3f}\")\n",
    "\n",
    "    # Apply colormap\n",
    "    rgba = cmap(norm(arr))\n",
    "    # Drop alpha channel and convert to uint8\n",
    "    rgba = (rgba[..., :3] * 255).astype(np.uint8)\n",
    "\n",
    "    # Overlay Geoboundary if provided\n",
    "    if geoboundary_gdf is not None:\n",
    "        print(\"Overlaying geoboundary...\")\n",
    "        try:\n",
    "            boundary_reprojected = geoboundary_gdf.to_crs(raster_crs)\n",
    "\n",
    "            boundary_mask = rasterize(\n",
    "                shapes = boundary_reprojected.geometry.boundary,\n",
    "                out_shape = (out_h, out_w),\n",
    "                transform = new_transform,\n",
    "                fill = 0,\n",
    "                default_value = 1,\n",
    "                dtype = np.uint8\n",
    "            ).astype(bool)\n",
    "\n",
    "            # Dilate the boundary to make it thicker\n",
    "            if boundary_width > 1:\n",
    "                boundary_mask = binary_dilation(boundary_mask, iterations = boundary_width - 1)\n",
    "\n",
    "            # Overlay boundary on the RGBA image\n",
    "            rgba[boundary_mask] = boundary_color\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error overlaying geoboundary: {e}\")\n",
    "\n",
    "    # Adding title to the image\n",
    "    img = Image.fromarray(rgba)\n",
    "\n",
    "    # If a title is provided, draw it on the image\n",
    "    if title:\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        # Load font\n",
    "        try:\n",
    "            font = ImageFont.truetype(font_path, title_font_size)\n",
    "        except IOError:\n",
    "            print(f\"Could not load font at {font_path}. Using default font.\")\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        # Calculate text size and position\n",
    "        text_bbox = draw.textbbox((0, 0), title, font = font)\n",
    "        text_width = text_bbox[2] - text_bbox[0]\n",
    "        text_height = text_bbox[3] - text_bbox[1]\n",
    "\n",
    "        text_x = (img.width - text_width) // 2\n",
    "        text_y = 10  # 10 pixels from the top\n",
    "\n",
    "        # Draw text\n",
    "        draw.text((text_x, text_y), title, font = font, fill = title_color)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "    # --------------------------\n",
    "    out_png = png_dir / f\"{Path(tif_path).stem}_{out_h}_{out_w}.png\"\n",
    "    img.save(out_png)\n",
    "\n",
    "    if update_bar is not None:\n",
    "        update_bar()\n",
    "\n",
    "    print(f\"Saved thumbnail: {out_png.stem}\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# --------------------- Function to generate monthly flood maps from parquet files\n",
    "def generate_monthly_flood_maps(parquet_path, template_path, year, buffer_meters = 240, output_dir = None, update_bar = None):\n",
    "    print(f\"\\n\\nProcessing {parquet_path.name} for year {year}...\")\n",
    "\n",
    "    # Load template metadata from corresponding 240m buffer tif\n",
    "    template_geotiff_path = template_path\n",
    "    assert template_geotiff_path.exists(), f\"Template GeoTIFF {template_geotiff_path} does not exist. Only 240m or 80m buffer tif files are available.\"\n",
    "    assert template_geotiff_path.parent.stem == parquet_path.parent.stem, f\"Template GeoTIFF {template_geotiff_path} does not match parquet file {parquet_path}.\"\n",
    "    print(f\"Using template GeoTIFF: {template_geotiff_path.name}\")\n",
    "\n",
    "    with rasterio.open(template_geotiff_path) as src:\n",
    "        transform = src.transform\n",
    "        height = src.height\n",
    "        width = src.width\n",
    "        bounds = src.bounds\n",
    "        template_crs = src.crs\n",
    "        exclusion_mask = src.read(1) == 1  # Exclusion mask pixels\n",
    "\n",
    "\n",
    "    # Load parquet data using polars for efficiency\n",
    "    df = pl.scan_parquet(parquet_path)\n",
    "        \n",
    "    for month in range(1, 13):\n",
    "        print(f\"\\nGenerating flood map for {year}-{month:02d}...\")\n",
    "        # Filter data for the specific year and month\n",
    "        expr = (\n",
    "            (pl.col('year') == year) &\n",
    "            (pl.col('month') == month) &\n",
    "            (pl.col('dem_metric_2') < 10) &\n",
    "            (pl.col('soil_moisture_sca') > 1) &\n",
    "            (pl.col('soil_moisture_zscore') > 1) &\n",
    "            (pl.col('soil_moisture') > 20) &\n",
    "            (pl.col('temp') > 0) &\n",
    "            (pl.col('land_cover') != 60) &\n",
    "            (pl.col('edge_false_positives') == 0)     \n",
    "        )\n",
    "        monthly_df = df.filter(expr).collect()\n",
    "        if monthly_df.shape[0] == 0:\n",
    "            print(f\"No flood points for {year}-{month:02d}. \")\n",
    "            monthly_flood_map = np.zeros((height, width), dtype = 'uint8')\n",
    "\n",
    "        else:\n",
    "            # Convert to GeoDataFrame\n",
    "            print(\"Creating GeoDataFrame from filtered rows\")\n",
    "            monthly_df = monthly_df.to_pandas()\n",
    "            geometry = [Point(xy) for xy in zip(monthly_df['lon'], monthly_df['lat'])]\n",
    "            gdf = gpd.GeoDataFrame(monthly_df, geometry = geometry, crs = \"EPSG:4326\")\n",
    "            print(f\"Number of flood points: {len(gdf)}\")\n",
    "            monthly_df = None\n",
    "            geometry = None\n",
    "\n",
    "            # Buffer points if required\n",
    "            if buffer_meters and buffer_meters > 0:\n",
    "                print(f\"Buffering flood points by {buffer_meters} meters...\")\n",
    "                # Choose a CRS for buffering\n",
    "                if template_crs.is_geographic:\n",
    "                    print(\"Template CRS is geographic. Reprojecting gdf to EPSG:3857 for buffering...\")\n",
    "                    buf_crs = \"EPSG:3857\"\n",
    "                else:\n",
    "                    print(\"Template CRS is projected. Using template CRS for buffering...\")\n",
    "                    buf_crs = template_crs\n",
    "                \n",
    "                # Reproject gdf to buffer CRS\n",
    "                print(f\"Reprojecting gdf to {buf_crs} for buffering...\")\n",
    "                gdf = gdf.to_crs(buf_crs) # Units in meters\n",
    "\n",
    "                # Buffer in meters\n",
    "                print(f\"Buffering points by {buffer_meters} meters...\")\n",
    "                gdf['geom_buf'] = gdf.geometry.buffer(buffer_meters)\n",
    "\n",
    "                # Merge buffered geometries\n",
    "                union_geom = unary_union(gdf['geom_buf'])\n",
    "\n",
    "                # Reproject back to template CRS\n",
    "                print(f\"Reprojecting back to {template_crs}...\")\n",
    "                union_geom = gpd.GeoSeries([union_geom], crs = buf_crs).to_crs(template_crs).iloc[0]\n",
    "                gdf = None\n",
    "\n",
    "            else:\n",
    "                print(\"No buffering: rasterizing single-point blobs\")\n",
    "                gdf = gdf.to_crs(template_crs)\n",
    "                px = abs(transform.a)\n",
    "                half_px = px / 2.0\n",
    "                union_geom = [(pt.buffer(half_px)) for pt in gdf.geometry]\n",
    "                gdf = None\n",
    "\n",
    "                # print(\"No buffering applied.\")\n",
    "                # gdf = gdf.to_crs(template_crs)\n",
    "                # union_geom = unary_union(gdf.geometry)\n",
    "\n",
    "\n",
    "            # Rasterize the unioned geometry\n",
    "            print(\"Rasterizing buffered geometries...\")\n",
    "            shapes = union_geom if isinstance(union_geom, list) else [union_geom]\n",
    "            monthly_flood_map = rasterize(\n",
    "                shapes,\n",
    "                out_shape = (height, width),\n",
    "                transform = transform,\n",
    "                fill = 0, # Value 0 for non-flooded areas\n",
    "                default_value = 2, # Value 2 for flooded areas\n",
    "                all_touched = True,\n",
    "                dtype = 'uint8'\n",
    "            )\n",
    "        \n",
    "        # Apply exclusion mask\n",
    "        if exclusion_mask is not None:\n",
    "            print(\"Applying exclusion mask...\")\n",
    "            monthly_flood_map = np.where(exclusion_mask, 1, monthly_flood_map)\n",
    "\n",
    "        # Save to GeoTIFF\n",
    "        output_path = output_dir / f\"{year}\" / f\"{year}-{month:02d}_{parquet_path.parent.stem}_flood_map.tif\"\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Saving flood map to {output_path}...\")\n",
    "        meta = {\n",
    "            'driver': 'GTiff',\n",
    "            'height': height,\n",
    "            'width': width,\n",
    "            'count': 1,\n",
    "            'dtype': 'uint8',\n",
    "            'crs': template_crs,\n",
    "            'transform': transform,\n",
    "            'compress': 'lzw'\n",
    "        }\n",
    "        with rasterio.open(output_path, 'w', **meta) as dst:\n",
    "            dst.write(monthly_flood_map, 1)\n",
    "\n",
    "        print(f\"Flood pixels percent: {np.sum(monthly_flood_map == 2) / monthly_flood_map.size * 100:.4f}%\")\n",
    "        print(f\"Completed {year}-{month:02d}.\\n\")\n",
    "        \n",
    "    print(\"--\"*40)\n",
    "\n",
    "    if update_bar is not None:\n",
    "        update_bar()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# --------------------- Function to create mosaic from multiple GeoTIFF files\n",
    "def create_mosaic(geotiff_paths, output_path, update_bar = None):\n",
    "    print(f\"\\nCreating mosaic for {len(geotiff_paths)} GeoTIFF files...\")\n",
    "    # Read all GeoTIFF files\n",
    "    src_files_to_mosaic = []\n",
    "    for fp in geotiff_paths:\n",
    "        src = rasterio.open(fp)\n",
    "        src_files_to_mosaic.append(src)\n",
    "\n",
    "    # Quick check for consistent CRS\n",
    "    crs_set = set([src.crs for src in src_files_to_mosaic])\n",
    "    if len(crs_set) != 1:\n",
    "        raise ValueError(\"Input GeoTIFF files have different CRS. Please reproject them to a common CRS first.\")\n",
    "    \n",
    "    # Merge the GeoTIFF files\n",
    "    mosaic, out_trans = merge(src_files_to_mosaic, method = 'max')\n",
    "    out_meta = src_files_to_mosaic[0].meta.copy()\n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": mosaic.shape[1],\n",
    "        \"width\": mosaic.shape[2],\n",
    "        \"transform\": out_trans,\n",
    "        \"compress\": \"lzw\"\n",
    "    })\n",
    "\n",
    "    # Write the mosaic to disk\n",
    "    with rasterio.open(output_path, \"w\", **out_meta) as dest:\n",
    "        dest.write(mosaic)\n",
    "\n",
    "    print(f\"Mosaic created and saved to {output_path}\")\n",
    "\n",
    "    # Close all opened datasets\n",
    "    for src in src_files_to_mosaic:\n",
    "        src.close()\n",
    "\n",
    "    if update_bar is not None:\n",
    "        update_bar()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf0e053",
   "metadata": {},
   "source": [
    "#### Determining tile names from Kenya lat/lon coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d6f9108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiles covering Kenya: ['N00E033', 'N00E036', 'N00E039', 'N03E033', 'N03E036', 'N03E039', 'S03E033', 'S03E036', 'S03E039', 'S06E033', 'S06E036', 'S06E039']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Bounding box for Kenya\n",
    "min_lon, max_lon = 33.501, 41.974\n",
    "min_lat, max_lat = -5.002, 5.506\n",
    "\n",
    "# Calculate the starting latitude and longitude for the tiles\n",
    "start_lat = (min_lat // 3) * 3\n",
    "end_lat = (max_lat // 3) * 3\n",
    "start_lon = (min_lon // 3) * 3\n",
    "end_lon = (max_lon // 3) * 3\n",
    "\n",
    "\n",
    "\n",
    "# Generate the list of tile names covering Kenya\n",
    "tile_names = []\n",
    "for lat in range(int(start_lat), int(end_lat) + 3, 3):\n",
    "    for lon in range(int(start_lon), int(end_lon) + 3, 3):\n",
    "        # print(f\"\\nlat: {lat}, lon: {lon}\")\n",
    "        tile_name = tile_name_from_latlon(lat, lon)\n",
    "        # print(f\"tile_name: {tile_name}\")\n",
    "        tile_names.append(tile_name)\n",
    "        \n",
    "\n",
    "tile_names = natsorted(list(set(tile_names)))  # Remove duplicates\n",
    "print(\"Tiles covering Kenya:\", tile_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a2cbf",
   "metadata": {},
   "source": [
    "#### Download flood maps from AI4Good Hugging Face repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74a187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading data for tile: N03E039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Eric\\climax\\floods_diffusion\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading N03/N03E039/N03E039-240m-buffer.tif to flood_data\\N03E039-240m-buffer.tif\n",
      "Downloading N03/N03E039/N03E039-post-processing.parquet to flood_data\\N03E039-post-processing.parquet\n",
      "File flood_data\\N03E039-post-processing.parquet already exists. Skipping download.\n",
      "\n",
      "Downloading data for tile: S03E036\n",
      "Downloading S03/S03E036/S03E036-240m-buffer.tif to flood_data\\S03E036-240m-buffer.tif\n",
      "Downloading S03/S03E036/S03E036-post-processing.parquet to flood_data\\S03E036-post-processing.parquet\n",
      "File flood_data\\S03E036-post-processing.parquet already exists. Skipping download.\n",
      "\n",
      "Downloading data for tile: S06E039\n",
      "Downloading S06/S06E039/S06E039-240m-buffer.tif to flood_data\\S06E039-240m-buffer.tif\n",
      "Downloading S06/S06E039/S06E039-post-processing.parquet to flood_data\\S06E039-post-processing.parquet\n",
      "File flood_data\\S06E039-post-processing.parquet already exists. Skipping download.\n",
      "\n",
      "Downloading data for tile: S06E033\n",
      "Downloading S06/S06E033/S06E033-240m-buffer.tif to flood_data\\S06E033-240m-buffer.tif\n",
      "Downloading S06/S06E033/S06E033-post-processing.parquet to flood_data\\S06E033-post-processing.parquet\n",
      "File flood_data\\S06E033-post-processing.parquet already exists. Skipping download.\n",
      "\n",
      "Downloading data for tile: S06E036\n",
      "Downloading S06/S06E036/S06E036-240m-buffer.tif to flood_data\\S06E036-240m-buffer.tif\n",
      "Downloading S06/S06E036/S06E036-post-processing.parquet to flood_data\\S06E036-post-processing.parquet\n",
      "File flood_data\\S06E036-post-processing.parquet already exists. Skipping download.\n",
      "\n",
      "Downloading data for tile: N03E033\n",
      "Downloading N03/N03E033/N03E033-240m-buffer.tif to flood_data\\N03E033-240m-buffer.tif\n",
      "Downloading N03/N03E033/N03E033-post-processing.parquet to flood_data\\N03E033-post-processing.parquet\n",
      "File flood_data\\N03E033-post-processing.parquet already exists. Skipping download.\n",
      "\n",
      "Downloading data for tile: N03E036\n",
      "Downloading N03/N03E036/N03E036-240m-buffer.tif to flood_data\\N03E036-240m-buffer.tif\n",
      "Downloading N03/N03E036/N03E036-post-processing.parquet to flood_data\\N03E036-post-processing.parquet\n",
      "File flood_data\\N03E036-post-processing.parquet already exists. Skipping download.\n",
      "\n",
      "Downloading data for tile: N00E033\n",
      "Downloading N00/N00E033/N00E033-240m-buffer.tif to flood_data\\N00E033-240m-buffer.tif\n",
      "Downloading N00/N00E033/N00E033-post-processing.parquet to flood_data\\N00E033-post-processing.parquet\n",
      "File flood_data\\N00E033-post-processing.parquet already exists. Skipping download.\n",
      "\n",
      "Downloading data for tile: N00E039\n",
      "Downloading N00/N00E039/N00E039-240m-buffer.tif to flood_data\\N00E039-240m-buffer.tif\n",
      "Downloading N00/N00E039/N00E039-post-processing.parquet to flood_data\\N00E039-post-processing.parquet\n",
      "File flood_data\\N00E039-post-processing.parquet already exists. Skipping download.\n",
      "\n",
      "Downloading data for tile: N00E036\n",
      "Downloading N00/N00E036/N00E036-240m-buffer.tif to flood_data\\N00E036-240m-buffer.tif\n",
      "Downloading N00/N00E036/N00E036-post-processing.parquet to flood_data\\N00E036-post-processing.parquet\n",
      "File flood_data\\N00E036-post-processing.parquet already exists. Skipping download.\n",
      "\n",
      "Downloading data for tile: S03E033\n",
      "Downloading S03/S03E033/S03E033-240m-buffer.tif to flood_data\\S03E033-240m-buffer.tif\n",
      "Downloading S03/S03E033/S03E033-post-processing.parquet to flood_data\\S03E033-post-processing.parquet\n",
      "File flood_data\\S03E033-post-processing.parquet already exists. Skipping download.\n",
      "\n",
      "Downloading data for tile: S03E039\n",
      "Downloading S03/S03E039/S03E039-240m-buffer.tif to flood_data\\S03E039-240m-buffer.tif\n",
      "Downloading S03/S03E039/S03E039-post-processing.parquet to flood_data\\S03E039-post-processing.parquet\n",
      "File flood_data\\S03E039-post-processing.parquet already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "for tile in tile_names:\n",
    "    download_tile(\n",
    "        tile = tile,\n",
    "        download_240m_buffer_tif = True,\n",
    "        download_80m_buffer_tif = False,\n",
    "        download_post_processing_parquet = True,\n",
    "        download_recurrence_80m_buffer_tif = False,\n",
    "        local_dir = None,\n",
    "        overwrite = True\n",
    "    )\n",
    "    time.sleep(3)  # Sleep for 3 seconds between downloads to avoid rate limiting\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522c2ab",
   "metadata": {},
   "source": [
    "#### Merge flood maps into a single tiff for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2e141fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 GeoTIFF files.\n",
      "\n",
      "Creating mosaic for 12 GeoTIFF files...\n",
      "Mosaic created and saved to flood_data\\kenya_240m_buffer_flood_mosaic.tif\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Directory where the downloaded flood map tiles are stored\n",
    "data_dir = Path(\"flood_data\")\n",
    "assert data_dir.exists(), f\"Data directory {data_dir} does not exist.\"\n",
    "\n",
    "# List all downloaded 240m buffer TIFF files\n",
    "geotiff_files = natsorted(list(data_dir.rglob(\"*-240m-buffer.tif\")))\n",
    "print(f\"Found {len(geotiff_files)} GeoTIFF files.\")\n",
    "\n",
    "create_mosaic(\n",
    "    geotiff_paths = geotiff_files,\n",
    "    output_path = data_dir / \"kenya_240m_buffer_flood_mosaic.tif\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef35e15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped mosaic saved to flood_data\\kenya_240m_buffer_flood_clipped.tif\n"
     ]
    }
   ],
   "source": [
    "## Optional:Clip the mosaic to Kenya boundary\n",
    "output_mosaic_path = data_dir / \"kenya_240m_buffer_flood_mosaic.tif\"\n",
    "assert output_mosaic_path.exists(), f\"Mosaic file {output_mosaic_path} does not exist.\"\n",
    "\n",
    "\n",
    "kenya_coords = [[[33.501, -5.002], [41.974, -5.002], [41.974, 5.506], [33.501, 5.506], [33.501, -5.002]]]\n",
    "geoms = [{'type': 'Polygon', 'coordinates': kenya_coords}]\n",
    "\n",
    "output_clipped_path = data_dir / \"kenya_240m_buffer_flood_clipped.tif\"\n",
    "with rasterio.open(output_mosaic_path) as src:\n",
    "    out_image, out_transform = mask(src, geoms, crop = True)\n",
    "    clip_meta = src.meta.copy()\n",
    "\n",
    "clip_meta.update({\n",
    "    \"driver\": \"GTiff\",\n",
    "    \"height\": out_image.shape[1],\n",
    "    \"width\": out_image.shape[2],\n",
    "    \"transform\": out_transform,\n",
    "    \"compress\": \"lzw\"\n",
    "\n",
    "})\n",
    "\n",
    "with rasterio.open(output_clipped_path, \"w\", **clip_meta) as dest:\n",
    "    dest.write(out_image)\n",
    "\n",
    "print(f\"Clipped mosaic saved to {output_clipped_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfab625f",
   "metadata": {},
   "source": [
    "#### Create thumbnail of overall flood map mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f560c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing flood_data\\kenya_240m_buffer_flood_clipped.tif...\n",
      "Downsampling by scale factor: 61.57\n",
      "Original resolution in km: 0.019, New resolution in km: 1.143\n",
      "Overlaying geoboundary...\n",
      "Saved thumbnail: kenya_240m_buffer_flood_clipped_1024_825\n",
      "\n",
      "Processing flood_data\\kenya_240m_buffer_flood_mosaic.tif...\n",
      "Downsampling by scale factor: 70.31\n",
      "Original resolution in km: 0.019, New resolution in km: 1.305\n",
      "Overlaying geoboundary...\n",
      "Saved thumbnail: kenya_240m_buffer_flood_mosaic_1024_768\n"
     ]
    }
   ],
   "source": [
    "png_dir = data_dir / \"thumbnails\"\n",
    "png_dir.mkdir(parents = True, exist_ok = True)\n",
    "max_size = 1024 # Max width or height of thumbnail\n",
    "\n",
    "\n",
    "# Generate thumbnail for cropped and mosaicked flood map\n",
    "kenya_boundary = get_admin_boundary(iso = 'KEN', adm = 'ADM0', save_path = None)\n",
    "mosaic_clipped_tif = data_dir / \"kenya_240m_buffer_flood_clipped.tif\"\n",
    "mosaic_tif = data_dir / \"kenya_240m_buffer_flood_mosaic.tif\"\n",
    "tif_paths = [mosaic_clipped_tif, mosaic_tif]\n",
    "\n",
    "for idx in range(len(tif_paths)):\n",
    "    make_thumbnail(\n",
    "        tif_paths = tif_paths,\n",
    "        idx = idx,\n",
    "        png_dir = png_dir,\n",
    "        max_size = 1024,\n",
    "        geoboundary_gdf = kenya_boundary,\n",
    "        boundary_color = [0, 0, 0], # Black boundary\n",
    "        boundary_width = 1,\n",
    "       \n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d99a1a",
   "metadata": {},
   "source": [
    "## Extract monthly flood maps from parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c841257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 parquet files.\n"
     ]
    }
   ],
   "source": [
    "## Find parquet files\n",
    "data_dir = Path(\"flood_data\")\n",
    "assert data_dir.exists(), f\"Data directory {data_dir} does not exist.\"\n",
    "\n",
    "parquet_files = natsorted(list(data_dir.rglob(\"*-post-processing.parquet\")))\n",
    "print(f\"Found {len(parquet_files)} parquet files.\")\n",
    "\n",
    "\n",
    "## ---- Configuration ----\n",
    "output_dir = data_dir / \"monthly_flood_maps\"\n",
    "output_dir.mkdir(parents = True, exist_ok = True)\n",
    "years = natsorted(list(range(2015, 2025)))\n",
    "buffer_meters = 240  # Use 240m to buffer flood maps\n",
    "\n",
    "## Generate monthly flood maps in parallel\n",
    "\n",
    "with tqdm(total = len(parquet_files) * len(years), desc = \"Generating monthly flood maps\") as bar:\n",
    "    delayed_tasks = []\n",
    "    for parquet_path in parquet_files:\n",
    "        template_path = parquet_path.parent / f\"{parquet_path.parent.stem}-240m-buffer.tif\"\n",
    "        for year in years:\n",
    "            task = delayed(generate_monthly_flood_maps)(\n",
    "                parquet_path = parquet_path,\n",
    "                template_path = template_path,\n",
    "                year = year,\n",
    "                buffer_meters = buffer_meters,\n",
    "                output_dir = output_dir,\n",
    "                update_bar = bar.update\n",
    "            )\n",
    "            delayed_tasks.append(task)\n",
    "\n",
    "    compute(*delayed_tasks, scheduler = 'threads', num_workers = os.cpu_count()//2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b01f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the downloaded flood map tiles are stored\n",
    "data_dir = Path(\"flood_data\")\n",
    "assert data_dir.exists(), f\"Data directory {data_dir} does not exist.\"\n",
    "\n",
    "# Empty dictionary to hold year monthly geotiff paths\n",
    "years = natsorted(list(range(2015, 2025)))\n",
    "yearly_monthly_geotiffs = {year: [] for year in years}\n",
    "\n",
    "out_image_dir = data_dir / \"monthly_flood_map_mosaics\"\n",
    "out_image_dir.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "# Populate the dictionary\n",
    "for year in years:\n",
    "    # print(f\"\\n\\nCreating mosaic for year {year}...\")\n",
    "    for month in range(1, 13):\n",
    "        # print(f\"\\nCreating mosaic for {year}-{month:02d}...\")\n",
    "        monthly_geotiffs = (data_dir / \"monthly_flood_maps\" / f\"{year}\").rglob(f\"*{year}-{month:02d}_*_flood_map.tif\")\n",
    "        monthly_geotiffs = natsorted(list(monthly_geotiffs))\n",
    "        # print(f\"Found {len(monthly_geotiffs)} GeoTIFFs for {year}-{month:02d}.\")\n",
    "        yearly_monthly_geotiffs[year].append(monthly_geotiffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2892cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed, compute\n",
    "from tqdm import tqdm\n",
    "\n",
    "with tqdm(total = len(years) * 12, desc = \"Creating monthly flood map mosaics\") as bar:\n",
    "    delayed_tasks = []\n",
    "    for year in years:\n",
    "        for month_idx in range(12):\n",
    "            geotiff_paths = yearly_monthly_geotiffs[year][month_idx]\n",
    "            output_path = out_image_dir / f\"{year}-{month_idx+1:02d}_kenya_flood_mosaic.tif\"\n",
    "            task = delayed(create_mosaic)(\n",
    "                geotiff_paths = geotiff_paths,\n",
    "                output_path = output_path,\n",
    "                update_bar = bar.update\n",
    "            )\n",
    "            delayed_tasks.append(task)\n",
    "    compute(*delayed_tasks, scheduler = 'threads', num_workers = min(10, os.cpu_count()//2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac5f05",
   "metadata": {},
   "source": [
    "#### Now we save the flood maps as thumbnails for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e424fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating monthly flood map mosaic thumbnails: 100%|██████████| 120/120 [06:26<00:00,  3.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# Define dtata dir\n",
    "data_dir = Path(\"flood_data\")\n",
    "assert data_dir.exists(), f\"Data directory {data_dir} does not exist.\"\n",
    "\n",
    "output_dir = data_dir / \"monthly_flood_map_mosaics\"\n",
    "png_dir = data_dir / \"monthly_flood_map_mosaic_thumbnails\"\n",
    "png_dir.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "\n",
    "\n",
    "tif_paths = natsorted(list(output_dir.rglob(\"*.tif\")))\n",
    "kenya_boundary = get_admin_boundary(iso = 'KEN', adm = 'ADM0', save_path = None)\n",
    "with tqdm(total = len(tif_paths), desc = \"Creating monthly flood map mosaic thumbnails\") as bar:\n",
    "    delayed_tasks = []\n",
    "    for idx in range(len(tif_paths)):\n",
    "        title = f\"{tif_paths[idx].stem.split('_')[0]}\"\n",
    "        task = delayed(make_thumbnail)(\n",
    "            tif_paths = tif_paths,\n",
    "            idx = idx,\n",
    "            png_dir = png_dir,\n",
    "            max_size = 2**13,\n",
    "            geoboundary_gdf = kenya_boundary,\n",
    "            boundary_color = [0, 0, 0], # Black boundary\n",
    "            boundary_width = 1,\n",
    "            title = title,\n",
    "            title_color = 'black',\n",
    "            title_font_size = 32,\n",
    "            font_path='arial.ttf', # Path to a .ttf font file\n",
    "            update_bar = bar.update\n",
    "        )\n",
    "        delayed_tasks.append(task)\n",
    "    compute(*delayed_tasks, scheduler = 'threads', num_workers = min(10, os.cpu_count()//2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092604b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120 PNG files for timelapse creation.\n",
      "Creating gif...\n"
     ]
    }
   ],
   "source": [
    "import imageio.v3 as iio\n",
    "import glob\n",
    "\n",
    "## Create mp4 or gif\n",
    "data_dir = Path(\"flood_data\")\n",
    "png_dir = data_dir / \"monthly_flood_map_mosaic_thumbnails\"\n",
    "assert png_dir.exists(), f\"PNG directory {png_dir} does not exist.\"\n",
    "\n",
    "png_files = natsorted(list(png_dir.rglob(\"*.png\")))\n",
    "print(f\"Found {len(png_files)} PNG files for timelapse creation.\")\n",
    "\n",
    "def create_mp4_or_gif(png_files, output_path = \"flood_map_timelapse\", fps = 2, duration = 0.5, file_type = \"mp4\"):\n",
    "    images = [iio.imread(png) for png in png_files] # R, C, Channel\n",
    "\n",
    "    if file_type == \"mp4\":\n",
    "        print(\"Creating mp4...\")\n",
    "        iio.imwrite(f\"{output_path.as_posix()}.mp4\", images, fps = fps)\n",
    "\n",
    "    elif file_type == \"gif\":\n",
    "        print(\"Creating gif...\")\n",
    "        iio.imwrite(f\"{output_path.as_posix()}.gif\", images, duration = duration)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"file_type must be 'mp4' or 'gif'\")\n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "create_mp4_or_gif(png_files, output_path = data_dir / \"2015-2024_flood_map_timelapse\", fps = 2, duration = 1, file_type = \"gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "floods-diffusion (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
